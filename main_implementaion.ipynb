{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the implementation of the Research Paper:  A deep learning method DCWR with HANet for stock market prediction using news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load SavedModel format of ELMO model\n",
    "elmo1 = tf.saved_model.load(r'E:\\Semesters\\Fyp prepation\\ELMO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(r'AAPL_articals_list','rb') as f:\n",
    "    list_art = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_len=[]\n",
    "for art in list_art:\n",
    "    sen_len.append(len(art))\n",
    "max(sen_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.3114672008012"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengt =[]\n",
    "for i in list_art:\n",
    "    lengt.append(len(i))\n",
    "    \n",
    "sum = sum(lengt)\n",
    "avg = sum / len(list_art)\n",
    "avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_art[93])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'elmo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4259e4f83e34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melmo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"default\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_art\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"elmo\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'elmo' is not defined"
     ]
    }
   ],
   "source": [
    "embeddings = elmo.signatures[\"default\"](tf.constant(list_art[:1]))[\"elmo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class Embeddings:\n",
    "    \n",
    "    def get_elmo_embeddings(elmo , Progress, artical_list : list):\n",
    "        embeddings_list =[]\n",
    "        total_indexes = len(artical_list)\n",
    "        progress = Progress()   \n",
    "        \n",
    "        for count, artical in enumerate(artical_list):\n",
    "            start_time = time.time()\n",
    "            embeddings_list.append(elmo.signatures[\"default\"](tf.constant(artical))[\"elmo\"])\n",
    "            time_taken = time.time() - start_time\n",
    "            progress.print_progress(count,total_indexes,time_taken)\n",
    "\n",
    "            \n",
    "        return embeddings_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got the Word Embeddings of news Atricals in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996th/1997th Completed in: 8.80 seconds. | Estimated Time to Complete: 0.00 seconds.  | Total Time Spilled: 158.58 minutes.s.\r"
     ]
    }
   ],
   "source": [
    "from FunctionProgress1 import Progress\n",
    "\n",
    "\n",
    "X = Embeddings.get_elmo_embeddings(elmo1, Progress, list_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "max_length = max(embeddings.shape[1] for embeddings in X)\n",
    "\n",
    "# Pad the embeddings with zeros to make them all have the same shape\n",
    "padded_embeddings = []\n",
    "for embeddings in X:\n",
    "    num_sentences, original_length, embedding_size = embeddings.shape\n",
    "    padding_size = max_length - original_length\n",
    "    padding = np.zeros((num_sentences, padding_size, embedding_size))\n",
    "    padded_embeddings.append(np.concatenate([embeddings, padding], axis=1))\n",
    "embeddings = np.concatenate(padded_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences, max_length, embedding_size = embeddings.shape\n",
    "embeddings = embeddings.reshape(num_sentences, max_length * embedding_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>implementing the ICA to the word Embeddings in the cell below </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import FastICA\n",
    "from FunctionProgress1 import Progress\n",
    "\n",
    "\n",
    "# Assuming we have obtained the ELMO embeddings in a list of numpy arrays `embeddings` of shape (n_words, embedding_size)\n",
    "\n",
    "# Apply ICA on each embedding in the list to extract independent features\n",
    "ica = FastICA(n_components=31, random_state=0)\n",
    "ica_embeddings = []\n",
    "\n",
    "progress = Progress()\n",
    "total_indexes = len(embeddings)\n",
    "\n",
    "for count ,embedding in enumerate(embeddings):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Reshape the embedding to 2D array of shape (n_words, embedding_size)\n",
    "    X = np.reshape(embedding, (-1, embedding.shape[-1]))\n",
    "    # Apply ICA on the flattened embedding to extract independent features\n",
    "    X_ica = ica.fit_transform(X)\n",
    "    # Reshape the extracted features back to 2D array of shape (n_words, n_components)\n",
    "    ica_embeddings.append(X_ica)\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    progress.print_progress(count,total_indexes,time_taken)\n",
    "    \n",
    "# Convert the list of ICA embeddings to numpy array\n",
    "ica_embeddings = np.array(ica_embeddings) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(\"ICA_embeddings\", 'wb') as f:\n",
    "    pickle.dump(ica_embeddings,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"ICA_embeddings\", 'rb') as f:\n",
    "   ica_embeddings=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df2 = pd.read_csv('E:\\Semesters\\Fyp prepation\\Data_preprocessing\\Data_searching\\Apple_dataset.csv', index_col= False)\n",
    "df2.dropna(inplace=True)\n",
    "df2.reset_index(drop=True, inplace=True)\n",
    "dates = pd.to_datetime(df2['Data_Time']).dt.date\n",
    "\n",
    "date_embedding_mapping = dict(zip(dates, ica_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-01-03</td>\n",
       "      <td>1.130179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-01-04</td>\n",
       "      <td>1.141786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-01-05</td>\n",
       "      <td>1.151786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-01-06</td>\n",
       "      <td>1.152679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-01-07</td>\n",
       "      <td>1.236607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>2013-12-24</td>\n",
       "      <td>20.273930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>2013-12-26</td>\n",
       "      <td>20.139286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>2013-12-27</td>\n",
       "      <td>20.003214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>2013-12-30</td>\n",
       "      <td>19.804285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2264</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>20.036428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2265 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date      Close\n",
       "0     2005-01-03   1.130179\n",
       "1     2005-01-04   1.141786\n",
       "2     2005-01-05   1.151786\n",
       "3     2005-01-06   1.152679\n",
       "4     2005-01-07   1.236607\n",
       "...          ...        ...\n",
       "2260  2013-12-24  20.273930\n",
       "2261  2013-12-26  20.139286\n",
       "2262  2013-12-27  20.003214\n",
       "2263  2013-12-30  19.804285\n",
       "2264  2013-12-31  20.036428\n",
       "\n",
       "[2265 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_AAPL_STOCK = pd.read_csv(r'E:\\Semesters\\Fyp prepation\\Dataset\\AAPL_Stock_Dataset/AAPL.csv')\n",
    "df_AAPL_STOCK['Date'] = pd.to_datetime(df_AAPL_STOCK['Date']).dt.date\n",
    "df_AAPL_STOCK = df_AAPL_STOCK[['Date','Close']]\n",
    "df_AAPL_STOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock_price_embeddings = pd.merge(df_AAPL_STOCK, pd.DataFrame(list(date_embedding_mapping.items()),columns=['Date', 'embeddings']), on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-03-28</td>\n",
       "      <td>3.330000</td>\n",
       "      <td>[[-0.019289728, -0.025915716, -0.010203429, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-07-20</td>\n",
       "      <td>5.133929</td>\n",
       "      <td>[[-0.0015902107, 0.0024687697, -0.0030268093, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-28</td>\n",
       "      <td>7.117500</td>\n",
       "      <td>[[-0.0056771897, -0.0062624235, -0.0047936994,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>7.307857</td>\n",
       "      <td>[[0.036892436, 0.0042556855, 0.088577226, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-04-23</td>\n",
       "      <td>9.672500</td>\n",
       "      <td>[[-0.04697889, -0.04293068, -0.018973464, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>2013-11-19</td>\n",
       "      <td>18.555357</td>\n",
       "      <td>[[0.00060118176, -0.00432523, -0.012103775, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>2013-11-20</td>\n",
       "      <td>18.392857</td>\n",
       "      <td>[[-0.0033492912, 0.0006496103, -0.004905187, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>2013-11-21</td>\n",
       "      <td>18.612143</td>\n",
       "      <td>[[-0.06283923, 0.06635708, 0.05533373, 0.02343...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>2013-11-22</td>\n",
       "      <td>18.564285</td>\n",
       "      <td>[[-0.030148793, -0.04174973, 0.06181645, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>2013-11-25</td>\n",
       "      <td>18.705000</td>\n",
       "      <td>[[0.08711832, 0.017831294, -0.003196844, 0.002...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date      Close                                         embeddings\n",
       "0    2007-03-28   3.330000  [[-0.019289728, -0.025915716, -0.010203429, 0....\n",
       "1    2007-07-20   5.133929  [[-0.0015902107, 0.0024687697, -0.0030268093, ...\n",
       "2    2010-01-28   7.117500  [[-0.0056771897, -0.0062624235, -0.0047936994,...\n",
       "3    2010-02-26   7.307857  [[0.036892436, 0.0042556855, 0.088577226, -0.0...\n",
       "4    2010-04-23   9.672500  [[-0.04697889, -0.04293068, -0.018973464, -0.0...\n",
       "..          ...        ...                                                ...\n",
       "544  2013-11-19  18.555357  [[0.00060118176, -0.00432523, -0.012103775, 0....\n",
       "545  2013-11-20  18.392857  [[-0.0033492912, 0.0006496103, -0.004905187, 0...\n",
       "546  2013-11-21  18.612143  [[-0.06283923, 0.06635708, 0.05533373, 0.02343...\n",
       "547  2013-11-22  18.564285  [[-0.030148793, -0.04174973, 0.06181645, -0.03...\n",
       "548  2013-11-25  18.705000  [[0.08711832, 0.017831294, -0.003196844, 0.002...\n",
       "\n",
       "[549 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stock_price_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "def generate_time_series(embeddings_df, stock_prices_df,lookahead_days):\n",
    "    lookahead_columns = ['Close+' +str(i) for i in range(1, lookahead_days+1)]\n",
    "    \n",
    "    new_df = pd.DataFrame(columns=['embeddings', 'Close']+lookahead_columns)\n",
    "    \n",
    "    # stock_prices_df['Date'] = pd.to_datetime(stock_prices_df['Date'])\n",
    "    # stock_prices_df = stock_prices_df.sort_values(by='Date')\n",
    "    \n",
    "    for index, row in embeddings_df.iterrows():\n",
    "        date = row['Date']\n",
    "        embedding = row['embeddings']\n",
    "        \n",
    "        # Find the stock price of the date of the embedding\n",
    "        stock_price = stock_prices_df[stock_prices_df['Date'] == date]['Close'].values[0]\n",
    "        \n",
    "        lookahead_dates = [date + timedelta(days=i) for i in range(1, 8)]\n",
    "        lookahead_prices = []\n",
    "        \n",
    "        for lookahead_date in lookahead_dates:\n",
    "            if lookahead_date in stock_prices_df['Date'].values:\n",
    "                lookahead_price = stock_prices_df[stock_prices_df['Date'] == lookahead_date]['Close'].values[0]\n",
    "            else:\n",
    "                lookahead_price = lookahead_price  # Use the last available price\n",
    "            lookahead_prices.append(lookahead_price)\n",
    "        \n",
    "        # Create the dictionary of column values\n",
    "        columns_values = {'embeddings': embedding, 'Close': stock_price}\n",
    "        columns_values.update(zip(lookahead_columns, lookahead_prices))        \n",
    "        \n",
    "        # Append the data to the new dataframe\n",
    "        new_df = new_df.append(columns_values, ignore_index=True)\n",
    "    \n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embeddings</th>\n",
       "      <th>Close</th>\n",
       "      <th>Close+1</th>\n",
       "      <th>Close+2</th>\n",
       "      <th>Close+3</th>\n",
       "      <th>Close+4</th>\n",
       "      <th>Close+5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-0.019289728, -0.025915716, -0.010203429, 0....</td>\n",
       "      <td>3.330000</td>\n",
       "      <td>3.348214</td>\n",
       "      <td>3.318214</td>\n",
       "      <td>3.318214</td>\n",
       "      <td>3.318214</td>\n",
       "      <td>3.344643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-0.0015902107, 0.0024687697, -0.0030268093, ...</td>\n",
       "      <td>5.133929</td>\n",
       "      <td>3.366786</td>\n",
       "      <td>3.366786</td>\n",
       "      <td>5.132143</td>\n",
       "      <td>4.817500</td>\n",
       "      <td>4.902143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-0.0056771897, -0.0062624235, -0.0047936994,...</td>\n",
       "      <td>7.117500</td>\n",
       "      <td>6.859286</td>\n",
       "      <td>6.859286</td>\n",
       "      <td>6.859286</td>\n",
       "      <td>6.954643</td>\n",
       "      <td>6.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0.036892436, 0.0042556855, 0.088577226, -0.0...</td>\n",
       "      <td>7.307857</td>\n",
       "      <td>6.858929</td>\n",
       "      <td>6.858929</td>\n",
       "      <td>7.463929</td>\n",
       "      <td>7.458929</td>\n",
       "      <td>7.476071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-0.04697889, -0.04293068, -0.018973464, -0.0...</td>\n",
       "      <td>9.672500</td>\n",
       "      <td>7.819643</td>\n",
       "      <td>7.819643</td>\n",
       "      <td>9.625000</td>\n",
       "      <td>9.358571</td>\n",
       "      <td>9.342857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>[[0.00060118176, -0.00432523, -0.012103775, 0....</td>\n",
       "      <td>18.555357</td>\n",
       "      <td>18.392857</td>\n",
       "      <td>18.612143</td>\n",
       "      <td>18.564285</td>\n",
       "      <td>18.564285</td>\n",
       "      <td>18.564285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>[[-0.0033492912, 0.0006496103, -0.004905187, 0...</td>\n",
       "      <td>18.392857</td>\n",
       "      <td>18.612143</td>\n",
       "      <td>18.564285</td>\n",
       "      <td>18.564285</td>\n",
       "      <td>18.564285</td>\n",
       "      <td>18.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>[[-0.06283923, 0.06635708, 0.05533373, 0.02343...</td>\n",
       "      <td>18.612143</td>\n",
       "      <td>18.564285</td>\n",
       "      <td>18.564285</td>\n",
       "      <td>18.564285</td>\n",
       "      <td>18.705000</td>\n",
       "      <td>19.049999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>[[-0.030148793, -0.04174973, 0.06181645, -0.03...</td>\n",
       "      <td>18.564285</td>\n",
       "      <td>19.498571</td>\n",
       "      <td>19.498571</td>\n",
       "      <td>18.705000</td>\n",
       "      <td>19.049999</td>\n",
       "      <td>19.498571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>[[0.08711832, 0.017831294, -0.003196844, 0.002...</td>\n",
       "      <td>18.705000</td>\n",
       "      <td>19.049999</td>\n",
       "      <td>19.498571</td>\n",
       "      <td>19.498571</td>\n",
       "      <td>19.859644</td>\n",
       "      <td>19.859644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            embeddings      Close    Close+1  \\\n",
       "0    [[-0.019289728, -0.025915716, -0.010203429, 0....   3.330000   3.348214   \n",
       "1    [[-0.0015902107, 0.0024687697, -0.0030268093, ...   5.133929   3.366786   \n",
       "2    [[-0.0056771897, -0.0062624235, -0.0047936994,...   7.117500   6.859286   \n",
       "3    [[0.036892436, 0.0042556855, 0.088577226, -0.0...   7.307857   6.858929   \n",
       "4    [[-0.04697889, -0.04293068, -0.018973464, -0.0...   9.672500   7.819643   \n",
       "..                                                 ...        ...        ...   \n",
       "544  [[0.00060118176, -0.00432523, -0.012103775, 0....  18.555357  18.392857   \n",
       "545  [[-0.0033492912, 0.0006496103, -0.004905187, 0...  18.392857  18.612143   \n",
       "546  [[-0.06283923, 0.06635708, 0.05533373, 0.02343...  18.612143  18.564285   \n",
       "547  [[-0.030148793, -0.04174973, 0.06181645, -0.03...  18.564285  19.498571   \n",
       "548  [[0.08711832, 0.017831294, -0.003196844, 0.002...  18.705000  19.049999   \n",
       "\n",
       "       Close+2    Close+3    Close+4    Close+5  \n",
       "0     3.318214   3.318214   3.318214   3.344643  \n",
       "1     3.366786   5.132143   4.817500   4.902143  \n",
       "2     6.859286   6.859286   6.954643   6.995000  \n",
       "3     6.858929   7.463929   7.458929   7.476071  \n",
       "4     7.819643   9.625000   9.358571   9.342857  \n",
       "..         ...        ...        ...        ...  \n",
       "544  18.612143  18.564285  18.564285  18.564285  \n",
       "545  18.564285  18.564285  18.564285  18.705000  \n",
       "546  18.564285  18.564285  18.705000  19.049999  \n",
       "547  19.498571  18.705000  19.049999  19.498571  \n",
       "548  19.498571  19.498571  19.859644  19.859644  \n",
       "\n",
       "[549 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_series_emb=  generate_time_series(df_stock_price_embeddings, df_AAPL_STOCK, 5)\n",
    "df_time_series_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>In the cells below we are trying to construct and train the HANet Network, but we failed to do so.\n",
    "We took multiple approach to construct the network, so the below cells are all messed up.\n",
    "\n",
    "We got the word embeddings, we applied the ICA on the word Embeddings, then we got the time series data of ica applied embeddings and stock prices.\n",
    "\n",
    "The only thing left is the HANet network construction and traning.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(AdditiveAttention, self).__init__(**kwargs)\n",
    "        self.W_q = self.add_weight(shape=(units, units), initializer='uniform', trainable=True)\n",
    "        self.W_k = self.add_weight(shape=(units, units), initializer='uniform', trainable=True)\n",
    "        self.V = self.add_weight(shape=(units, 1), initializer='uniform', trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        q = tf.matmul(inputs, self.W_q)\n",
    "        k = tf.matmul(inputs, self.W_k)\n",
    "        attn_scores = tf.matmul(tf.nn.tanh(q + k), self.V)\n",
    "        attn_scores = tf.nn.softmax(attn_scores, axis=1)\n",
    "        output = tf.reduce_sum(inputs * attn_scores, axis=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "class HANetModel(tf.keras.Model):\n",
    "    def __init__(self, sentence_length, embedding_dim, hidden_units, num_future_days, attention_units):\n",
    "        # disable_eager_execution()\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "        super(HANetModel, self).__init__()\n",
    "\n",
    "        # Sentence-level input and attention layers\n",
    "        self.sentence_input = tf.keras.layers.Input(shape=(sentence_length, embedding_dim))\n",
    "        self.sentence_lstm = tf.keras.layers.LSTM(units=hidden_units, return_sequences=True)(self.sentence_input)\n",
    "        self.sentence_attention = AdditiveAttention(units=attention_units)\n",
    "\n",
    "        # Flatten layer for stock price prediction\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        # Dense layers for stock price prediction\n",
    "        self.forecast = tf.keras.layers.Dense(units=num_future_days)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        sentence_inputs = inputs\n",
    "\n",
    "        # Apply the sentence attention layer\n",
    "        sentence_attention = self.sentence_attention(self.sentence_lstm)\n",
    "\n",
    "        # Flatten and Dense layer for stock price forecasting\n",
    "        flatten = self.flatten(sentence_attention)\n",
    "        forecast = self.forecast(flatten)\n",
    "\n",
    "        return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(tf.keras.layers.Layer):\n",
    "    def call(self, y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame 'df_time_series_emb' with a column 'embeddings' containing word embeddings (as numpy arrays)\n",
    "\n",
    "# Step 1: Convert the 'embeddings' column to a list of numpy arrays\n",
    "sequences = df_time_series_emb['embeddings'].tolist()\n",
    "\n",
    "# Step 2: Find vocab_size, sentence_length, and word_length\n",
    "vocab_size = len(sequences)\n",
    "sentence_length = sequences[0].shape[0]  # Assuming the word embeddings are 2D arrays with consistent dimensions\n",
    "word_length = sequences[0].shape[1]\n",
    "embedding_dim = sequences[0].shape[1]  # Same as word_length\n",
    "hidden_units = embedding_dim\n",
    "num_future_days =5\n",
    "# Now you have the values for YOUR_VOCAB_SIZE, YOUR_MAX_SENTENCE_LENGTH, YOUR_MAX_WORD_LENGTH, and YOUR_EMBEDDING_DIMENSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile the model\n",
    "model = HANetModel(sentence_length, embedding_dim, hidden_units, num_future_days)\n",
    "model.compile(optimizer='adam', loss=MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "sentence_embeddings_data =df_time_series_emb['embeddings'].tolist()\n",
    "max_sentence_length = max(len(seq) for seq in sentence_embeddings_data)\n",
    "\n",
    "sentence_embeddings_data = pad_sequences(sentence_embeddings_data, maxlen=max_sentence_length, dtype=np.float32)\n",
    "\n",
    "sentence_embeddings_data = np.array(sentence_embeddings_data, dtype=np.float32)\n",
    "\n",
    "# current_stock_price = np.array(df_time_series_emb['Close'])\n",
    "future_stock_prices = np.array(df_time_series_emb.iloc[:, -6:])  # Keep only the last 5 days' stock prices\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentence_embeddings_data, future_stock_prices, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ReshapeLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, target_shape, **kwargs):\n",
    "        super(ReshapeLayer, self).__init__(**kwargs)\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.reshape(inputs, self.target_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# X_train_flat = np.array([np.array(x) for x in X_train])\n",
    "\n",
    "\n",
    "# Convert NumPy arrays to TensorFlow tensors\n",
    "X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "y_train_tf = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "X_test_tf = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "y_test_tf = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# Reshape y_train_tf and y_test_tf to have shape (None, num_future_days)\n",
    "reshape_layer = ReshapeLayer(target_shape=(-1, num_future_days))\n",
    "y_train_tf = reshape_layer(y_train_tf)\n",
    "y_test_tf = reshape_layer(y_test_tf)\n",
    "num_samples = X_train_tf.shape[0]\n",
    "steps_per_epoch = num_samples\n",
    "\n",
    "\n",
    "hanet_model = HANetModel(sentence_length=11484, embedding_dim=31, hidden_units=64, num_future_days=5, attention_units= 64)\n",
    "hanet_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Create a TensorFlow session and initialize variables\n",
    "sess = tf.compat.v1.Session()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "# Now you can use the TensorFlow tensors in the model.fit() function\n",
    "\n",
    "# hanet_model.fit(X_train_tf, y_train_tf, epochs=10, batch_size=32, steps_per_epoch=steps_per_epoch,validation_data=(X_test_tf, y_test_tf), session=sess)\n",
    "\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = hanet_model.predict(X_test_tf, session=sess)\n",
    "# print(predictions)  # This will give you the predicted future stock prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([439, 5])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, epochs=10, batch_size=32):\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss=MeanSquaredError(), metrics=['mean_squared_error'])\n",
    "\n",
    "    # Convert data to tensors\n",
    "    X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "    y_train_tf = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "    X_test_tf = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "    y_test_tf = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "    \n",
    "    # Reshape y_train_tf and y_test_tf to have shape (None, num_future_days)\n",
    "    y_train_tf = tf.reshape(y_train_tf, (-1, num_future_days))\n",
    "    y_test_tf = tf.reshape(y_test_tf, (-1, num_future_days))\n",
    "    # Train the model\n",
    "    model.fit(X_train_tf, y_train_tf, epochs=epochs, batch_size=batch_size, validation_data=(X_test_tf, y_test_tf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 239, in __call__\n        self._loss_metric.update_state(\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 70, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 449, in update_state  **\n        sample_weight = tf.__internal__.ops.broadcast_weights(\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\keras_tensor.py\", line 254, in __array__\n        raise TypeError(\n\n    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='Placeholder:0', description=\"created by layer 'tf.cast_14'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-126-ae9f526f9f6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-125-dc62ac3442b8>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, X_train, y_train, X_test, y_test, epochs, batch_size)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0my_test_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_tf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_future_days\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_tf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_tf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_tf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 239, in __call__\n        self._loss_metric.update_state(\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 70, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 449, in update_state  **\n        sample_weight = tf.__internal__.ops.broadcast_weights(\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\keras_tensor.py\", line 254, in __array__\n        raise TypeError(\n\n    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='Placeholder:0', description=\"created by layer 'tf.cast_14'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.\n"
     ]
    }
   ],
   "source": [
    "model = HANetModel(sentence_length, embedding_dim, hidden_units, num_future_days)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, X_train, y_train, X_test, y_test, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W_q = self.add_weight(name='W_q',\n",
    "                                   shape=(input_shape[-1], input_shape[-1]),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.W_k = self.add_weight(name='W_k',\n",
    "                                   shape=(input_shape[-1], input_shape[-1]),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.W_v = self.add_weight(name='W_v',\n",
    "                                   shape=(input_shape[-1], input_shape[-1]),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        q = tf.matmul(inputs, self.W_q)\n",
    "        k = tf.matmul(inputs, self.W_k)\n",
    "        v = tf.matmul(inputs, self.W_v)\n",
    "\n",
    "        attn_scores = tf.matmul(q, k, transpose_b=True)\n",
    "        attn_scores = tf.nn.softmax(attn_scores)\n",
    "        output = tf.matmul(attn_scores, v)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HANetModel(tf.keras.Model):\n",
    "    def __init__(self, sentence_length, embedding_dim, hidden_units, num_future_days):\n",
    "        super(HANetModel, self).__init__()\n",
    "\n",
    "        # Sentence-level input and attention layers\n",
    "        self.sentence_input = tf.keras.layers.Input(shape=(sentence_length, embedding_dim))\n",
    "        self.sentence_lstm = tf.keras.layers.LSTM(units=hidden_units, return_sequences=True)(self.sentence_input)\n",
    "        self.sentence_attention = AttentionLayer()(self.sentence_lstm)\n",
    "\n",
    "        # Flatten layer for stock price prediction\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        # Dense layers for stock price prediction\n",
    "        self.forecast = tf.keras.layers.Dense(units=num_future_days)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        sentence_inputs = inputs\n",
    "\n",
    "        # Apply the sentence attention layer\n",
    "        sentence_attention = self.sentence_attention(sentence_inputs)\n",
    "\n",
    "        # Flatten and Dense layer for stock price forecasting\n",
    "        flatten = self.flatten(sentence_attention)\n",
    "        forecast = self.forecast(flatten)\n",
    "\n",
    "        return forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have defined all the necessary variables such as sentence_length, embedding_dim, hidden_units, and num_future_days.\n",
    "\n",
    "# Create an instance of the HANetModel\n",
    "model = HANetModel(sentence_length, embedding_dim, hidden_units, num_future_days)\n",
    "\n",
    "# Compile the model with an appropriate optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_function = tf.keras.losses.MeanSquaredError()\n",
    "model.compile(optimizer=optimizer, loss=loss_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"model_3\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 11484, 31) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-152-9c163a49ed51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Train the model with your training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"model_3\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 11484, 31) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have X_train and y_train as your training data, and X_test and y_test as your test data.\n",
    "\n",
    "# Train the model with your training data\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-90-8735d9a5aa78>:71: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train_seq = np.array(sequences)  # Assuming you have the training sequences for word embeddings\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense, Attention, Concatenate, RepeatVector, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have the sequences of word embeddings and stock data features\n",
    "# sequences: List of word embeddings for news articles\n",
    "# stock_data: 2D array containing historical stock data features\n",
    "# labels: 1D array containing the corresponding stock prices for the future days\n",
    "\n",
    "# Step 1: Preprocess the word embeddings (sequences) and stock data\n",
    "# sequences = ...\n",
    "# stock_data = ...\n",
    "# labels = ...\n",
    "\n",
    "# Step 2: Find vocab_size, sentence_length, and word_length\n",
    "# vocab_size = len(sequences)\n",
    "# sentence_length = sequences[0].shape[0]  # Assuming the word embeddings are 2D arrays with consistent dimensions\n",
    "# word_length = sequences[0].shape[1]\n",
    "# embedding_dim = sequences[0].shape[1]  # Same as word_length\n",
    "# hidden_units = embedding_dim\n",
    "# num_future_days = 5\n",
    "\n",
    "attention_units = 64  # Replace with the attention units\n",
    "num_stock_features = X_train.shape[1]  # Replace with the actual number of stock data features\n",
    "\n",
    "# Step 3: Define the HANetModel\n",
    "class HANetModel(tf.keras.Model):\n",
    "    def __init__(self, sentence_length, word_length, vocab_size, embedding_dim, hidden_units, num_future_days, attention_units, num_stock_features):\n",
    "        tf.config.experimental_run_functions_eagerly(True)\n",
    "        # tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\n",
    "        super(HANetModel, self).__init__()\n",
    "\n",
    "        # Sentence-level input and attention layers\n",
    "        self.sentence_input = Input(shape=(sentence_length, word_length))\n",
    "        self.sentence_lstm = LSTM(units=hidden_units, return_sequences=True)\n",
    "\n",
    "        # Stock data input and attention layers\n",
    "        self.stock_input = Input(shape=(num_stock_features,))\n",
    "        self.stock_attention = Dense(units=attention_units, activation='tanh')\n",
    "        self.stock_attention_repeated = RepeatVector(sentence_length)\n",
    "\n",
    "        # Attention fusion\n",
    "        self.attention_fusion = Concatenate(axis=-1)\n",
    "\n",
    "        # Flatten and Dense layer for stock price forecasting\n",
    "        self.flatten = Flatten()\n",
    "        self.forecast = Dense(units=num_future_days)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        sentence_inputs, stock_inputs = inputs\n",
    "\n",
    "        # Apply the sentence LSTM\n",
    "        sentence_lstm = self.sentence_lstm(self.sentence_input)\n",
    "\n",
    "        # Apply the stock attention layer\n",
    "        stock_attention = self.stock_attention(self.stock_input)\n",
    "        stock_attention_repeated = self.stock_attention_repeated(stock_attention)\n",
    "\n",
    "        # Attention fusion\n",
    "        attention_fusion = self.attention_fusion([sentence_lstm, stock_attention_repeated])\n",
    "\n",
    "        # Flatten and Dense layer for stock price forecasting\n",
    "        flatten = self.flatten(attention_fusion)\n",
    "        forecast = self.forecast(flatten)\n",
    "\n",
    "        return forecast\n",
    "# Step 4: Convert NumPy arrays to TensorFlow tensors\n",
    "X_train_seq = np.array(sequences)  # Assuming you have the training sequences for word embeddings\n",
    "X_train_stock = np.array(X_train)  # Assuming you have the training stock data features\n",
    "y_train = np.array(y_train)  # Assuming you have the training stock prices for the future days\n",
    "\n",
    "\n",
    "attention_units = 64  # Replace with the attention units\n",
    "num_stock_features = X_train_stock.shape[1]  # R\n",
    "\n",
    "# Step 5: Instantiate the model\n",
    "# hanet_model = HANetModel(sentence_length, word_length, vocab_size, embedding_dim, hidden_units, num_future_days, attention_units, num_stock_features)\n",
    "\n",
    "# # Step 6: Compile and train the model\n",
    "# hanet_model.compile(optimizer='adam', loss='mean_squared_error') \n",
    "# hanet_model.fit(x=[X_train_seq, X_train_stock], y=y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_seq: (549,)\n",
      "Shape of X_train_stock: (549, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-76-2941b7049e5d>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train_seq = np.array(embeddings_column.tolist())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "549"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have your DataFrame named df_time_series_emb\n",
    "# Extract the embeddings and stock data columns\n",
    "embeddings_column =  df_time_series_emb['embeddings']\n",
    "\n",
    "stock_prices =  df_time_series_emb[['Close','Close+1', 'Close+2', 'Close+3', 'Close+4', 'Close+5']]\n",
    "\n",
    "# Convert the embeddings column to a numpy array\n",
    "X_train_seq = np.array(embeddings_column.tolist())\n",
    "\n",
    "# Convert the stock data columns to a numpy array\n",
    "# X_train_stock = np.array(future_stock_prices)\n",
    "y_train = np.array(stock_prices)\n",
    "\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(\"Shape of X_train_seq:\", X_train_seq.shape)  # Should be (number of samples, sentence_length, word_length)\n",
    "print(\"Shape of X_train_stock:\", y_train.shape)  # Should be (number of samples, number of stock data features)\n",
    "\n",
    "len(embeddings_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "You are attempting to use Python control flow in a layer that was not declared to be dynamic. Pass `dynamic=True` to the class constructor.\nEncountered error:\n\"\"\"\nin user code:\n\n    File \"<ipython-input-85-e5a7cd413566>\", line 53, in call  *\n        sentence_inputs, stock_inputs = inputs\n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\n\n\"\"\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-455f43eb1b3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-110-455f43eb1b3c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mhanet_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 776\u001b[1;33m     return func.fit(\n\u001b[0m\u001b[0;32m    777\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training_generator_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    735\u001b[0m     batch_size = model._validate_or_infer_batch_size(batch_size,\n\u001b[0;32m    736\u001b[0m                                                      steps_per_epoch, x)\n\u001b[1;32m--> 737\u001b[1;33m     x, y, sample_weights = model._standardize_user_data(\n\u001b[0m\u001b[0;32m    738\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2290\u001b[0m     \u001b[1;31m# First, we build the model on the fly if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2291\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2292\u001b[1;33m       \u001b[0mall_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_model_with_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2293\u001b[0m       \u001b[0mis_build_called\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2294\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_build_model_with_inputs\u001b[1;34m(self, inputs, targets)\u001b[0m\n\u001b[0;32m   2517\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2518\u001b[0m       \u001b[0mcast_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2519\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2520\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mprocessed_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_dict_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[1;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[0;32m   2605\u001b[0m           \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2606\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2607\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2608\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2609\u001b[0m         \u001b[1;31m# This Model or a submodel is dynamic and hasn't overridden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abdul\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer_v1.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 766\u001b[1;33m               raise TypeError('You are attempting to use Python control '\n\u001b[0m\u001b[0;32m    767\u001b[0m                               \u001b[1;34m'flow in a layer that was not declared to be '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m                               \u001b[1;34m'dynamic. Pass `dynamic=True` to the class '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: You are attempting to use Python control flow in a layer that was not declared to be dynamic. Pass `dynamic=True` to the class constructor.\nEncountered error:\n\"\"\"\nin user code:\n\n    File \"<ipython-input-85-e5a7cd413566>\", line 53, in call  *\n        sentence_inputs, stock_inputs = inputs\n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\n\n\"\"\""
     ]
    }
   ],
   "source": [
    "# Define the HANet model\n",
    "hanet_model = HANetModel(sentence_length, word_length, vocab_size, embedding_dim, hidden_units, num_future_days, attention_units, num_stock_features)\n",
    "\n",
    "# Compile the model\n",
    "hanet_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "# Train the model\n",
    "# tf.config.experimental_run_functions_eagerly(True)\n",
    "@tf.function\n",
    "def train():\n",
    "    hanet_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = np.concatenate(X_train_seq, axis=0)  # Convert to a single 3D array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(439, 11484, 31)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([549, 6])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock Data Fusion:\n",
    "\n",
    "# Concatenate the sentence-level representations obtained from the attention mechanism with the corresponding stock data features. This creates a joint representation of the news articles and stock data for each time step.\n",
    "# Forecasting:\n",
    "\n",
    "# Apply fully connected layers on top of the fused representations to perform stock price prediction. The output layer will produce the predicted stock prices for the next time steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30424d9308010dafecca22b5ec9402eef0e44005ea326a16069b35614aa41d25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
